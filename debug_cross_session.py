#!/usr/bin/env python3
"""
è°ƒè¯•è·¨sessionè®¿é—®æ—¶çš„å†…å®¹é—®é¢˜
"""

import requests
import json
import time
import re

def debug_cross_session():
    """è¯¦ç»†è°ƒè¯•è·¨sessionå†…å®¹ä¼ é€’"""
    base_url = 'http://localhost:5001'
    
    print("ğŸ” è°ƒè¯•è·¨sessionå†…å®¹ä¼ é€’é—®é¢˜")
    print("=" * 50)
    
    session1 = requests.Session()
    
    # ä½¿ç”¨å·²çŸ¥æˆåŠŸçš„session
    known_session_id = "365a3ce6-8c6b-4e3a-b249-3f5b460efe3b"
    
    print(f"1ï¸âƒ£ ä½¿ç”¨å·²çŸ¥sessionè®¿é—®: {known_session_id}")
    
    # ç›´æ¥è®¿é—®åˆ†æé¡µé¢
    session2 = requests.Session()
    analysis_url = f"{base_url}/analysis/research_topic?sid={known_session_id}"
    
    print(f"   URL: {analysis_url}")
    
    analysis_response = session2.get(analysis_url)
    
    if analysis_response.status_code == 200:
        content = analysis_response.text
        print(f"âœ… é¡µé¢è®¿é—®æˆåŠŸ")
        
        # è¯¦ç»†åˆ†æé¡µé¢å†…å®¹
        print(f"\n2ï¸âƒ£ åˆ†æé¡µé¢å†…å®¹ç»“æ„...")
        
        # æŸ¥æ‰¾è®ºæ–‡æ•°é‡
        paper_count_match = re.search(r'åŸºäº\s*(\d+)\s*ç¯‡ç›¸å…³æ–‡çŒ®', content)
        if paper_count_match:
            found_count = paper_count_match.group(1)
            print(f"   è®ºæ–‡æ•°é‡: {found_count} ç¯‡")
        
        # æŸ¥æ‰¾åˆ†æå†…å®¹åŒºåŸŸ
        pre_matches = re.findall(r'<pre[^>]*>(.*?)</pre>', content, re.DOTALL)
        
        if pre_matches:
            for i, match in enumerate(pre_matches):
                cleaned_content = match.strip()
                print(f"   <pre>å— {i+1}: é•¿åº¦={len(cleaned_content)}")
                print(f"   å†…å®¹é¢„è§ˆ: {cleaned_content[:200]}...")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„AIå†…å®¹
                if len(cleaned_content) > 100:
                    if "é€‰é¢˜å»ºè®®" in cleaned_content or "ç ”ç©¶æ–¹å‘" in cleaned_content:
                        print(f"   âœ… åŒ…å«AIåˆ†æå†…å®¹")
                    elif "åŸºäºURLå‚æ•°æ¢å¤" in cleaned_content:
                        print(f"   âŒ æ˜¾ç¤ºå¤‡ç”¨æ¢å¤å†…å®¹")
                    else:
                        print(f"   âš ï¸  å†…å®¹ç±»å‹ä¸æ˜ç¡®")
                else:
                    print(f"   âŒ å†…å®¹è¿‡çŸ­")
        else:
            print(f"   âŒ æ²¡æœ‰æ‰¾åˆ°<pre>æ ‡ç­¾å†…å®¹")
        
        # æ£€æŸ¥é¡µé¢æ˜¯å¦è§¦å‘äº†æ¢å¤é€»è¾‘
        if "åŸºäºURLå‚æ•°æ¢å¤" in content:
            print(f"\n   âš ï¸  é¡µé¢è§¦å‘äº†å¤‡ç”¨æ¢å¤é€»è¾‘")
            print(f"   è¿™æ„å‘³ç€sessionç¼“å­˜ä¸­æ²¡æœ‰æ‰¾åˆ°å®Œæ•´çš„åˆ†æç»“æœ")
        
        # æŸ¥æ‰¾æ¨¡æ¿å˜é‡çš„å€¼
        print(f"\n3ï¸âƒ£ æ£€æŸ¥æ¨¡æ¿å˜é‡...")
        
        # æŸ¥æ‰¾analysis_resultå˜é‡çš„å®é™…å€¼
        analysis_result_pattern = r'<pre[^>]*>([^<]*)</pre>'
        analysis_match = re.search(analysis_result_pattern, content, re.DOTALL)
        
        if analysis_match:
            actual_analysis_result = analysis_match.group(1).strip()
            print(f"   analysis_resulté•¿åº¦: {len(actual_analysis_result)}")
            print(f"   analysis_resultå†…å®¹: {actual_analysis_result[:300]}...")
        
    else:
        print(f"âŒ é¡µé¢è®¿é—®å¤±è´¥: {analysis_response.status_code}")
    
    # æ£€æŸ¥sessionç¼“å­˜çŠ¶æ€
    print(f"\n4ï¸âƒ£ æ£€æŸ¥sessionç¼“å­˜çŠ¶æ€...")
    
    try:
        from utils.session_cache import get_papers_cache
        cache = get_papers_cache()
        
        # æ£€æŸ¥papersç¼“å­˜
        papers_data = cache.get_papers(known_session_id)
        if papers_data:
            print(f"   âœ… Papersç¼“å­˜å­˜åœ¨")
            print(f"      è®ºæ–‡æ•°é‡: {len(papers_data.get('papers', []))}")
            print(f"      æŸ¥è¯¢: {papers_data.get('query', 'N/A')}")
        else:
            print(f"   âŒ Papersç¼“å­˜ä¸å­˜åœ¨")
        
        # æ£€æŸ¥åˆ†æç»“æœç¼“å­˜
        analysis_data = cache.get_analysis_result(known_session_id, 'research_topic')
        if analysis_data:
            print(f"   âœ… åˆ†æç»“æœç¼“å­˜å­˜åœ¨")
            print(f"      è®ºæ–‡æ•°é‡: {analysis_data.get('paper_count')}")
            print(f"      å†…å®¹é•¿åº¦: {len(analysis_data.get('content', ''))}")
            print(f"      å†…å®¹é¢„è§ˆ: {analysis_data.get('content', '')[:200]}...")
            
            # åˆ†æè¿™ä¸ªç¼“å­˜çš„å†…å®¹æ˜¯å¦æœ‰æ•ˆ
            cached_content = analysis_data.get('content', '')
            if len(cached_content) > 500:
                print(f"   âœ… ç¼“å­˜å†…å®¹å……è¶³")
            else:
                print(f"   âŒ ç¼“å­˜å†…å®¹è¿‡çŸ­")
        else:
            print(f"   âŒ åˆ†æç»“æœç¼“å­˜ä¸å­˜åœ¨")
        
        # æ£€æŸ¥æ‰€æœ‰ç¼“å­˜çš„session
        cache_stats = cache.get_cache_stats()
        print(f"\n   ç¼“å­˜ç»Ÿè®¡:")
        print(f"      æ€»sessionæ•°: {cache_stats.get('total_sessions', 0)}")
        print(f"      æ€»è®ºæ–‡æ•°: {cache_stats.get('total_papers', 0)}")
        
        sessions_info = cache_stats.get('sessions', {})
        for sid, info in sessions_info.items():
            if sid == known_session_id:
                print(f"      âœ… ç›®æ ‡sessionå­˜åœ¨: {sid}")
                print(f"         è®ºæ–‡æ•°é‡: {info.get('paper_count', 0)}")
                print(f"         è®¿é—®æ¬¡æ•°: {info.get('access_count', 0)}")
            
    except Exception as e:
        print(f"   âŒ æ£€æŸ¥ç¼“å­˜å‡ºé”™: {e}")
    
    # æµ‹è¯•æ‰‹åŠ¨é‡å»ºåˆ†æç»“æœ
    print(f"\n5ï¸âƒ£ æµ‹è¯•æ‰‹åŠ¨åˆ›å»ºåˆ†æç»“æœ...")
    
    try:
        from utils.session_cache import get_papers_cache
        cache = get_papers_cache()
        
        # æ‰‹åŠ¨å­˜å‚¨ä¸€ä¸ªæµ‹è¯•åˆ†æç»“æœ
        test_result = {
            'content': '<h3>æµ‹è¯•AIåˆ†æå†…å®¹</h3><p>è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•çš„AIåˆ†æç»“æœï¼Œç”¨äºéªŒè¯è·¨sessionå­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½ã€‚</p><p>å¦‚æœæ‚¨çœ‹åˆ°è¿™ä¸ªå†…å®¹ï¼Œè¯´æ˜sessionç¼“å­˜çš„åˆ†æç»“æœå­˜å‚¨åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚</p>',
            'paper_count': 100,
            'search_query': 'test query',
            'timestamp': '2025-07-13 18:30:00',
            'loading': False,
            'error': None,
            'analysis_type': 'research_topic'
        }
        
        success = cache.store_analysis_result(known_session_id, 'research_topic', test_result)
        
        if success:
            print(f"   âœ… æ‰‹åŠ¨å­˜å‚¨æµ‹è¯•ç»“æœæˆåŠŸ")
            
            # ç«‹å³å°è¯•æ£€ç´¢
            retrieved = cache.get_analysis_result(known_session_id, 'research_topic')
            if retrieved:
                print(f"   âœ… ç«‹å³æ£€ç´¢æˆåŠŸ")
                print(f"      å†…å®¹é•¿åº¦: {len(retrieved.get('content', ''))}")
            else:
                print(f"   âŒ ç«‹å³æ£€ç´¢å¤±è´¥")
        else:
            print(f"   âŒ æ‰‹åŠ¨å­˜å‚¨å¤±è´¥")
            
    except Exception as e:
        print(f"   âŒ æ‰‹åŠ¨æµ‹è¯•å‡ºé”™: {e}")

if __name__ == "__main__":
    debug_cross_session()