#!/usr/bin/env python3
"""
ç»Ÿè®¡å„æœŸåˆŠè¿‘ä¸€å¹´çš„å¹³å‡å®¡ç¨¿å‘¨æœŸ
"""

import requests
import xml.etree.ElementTree as ET
import json
from datetime import datetime, timedelta
from collections import defaultdict
from typing import Dict, List, Any, Optional
import statistics

def analyze_journal_review_cycles():
    """åˆ†æå„æœŸåˆŠçš„å®¡ç¨¿å‘¨æœŸ"""
    
    print("=" * 100)
    print("æœŸåˆŠå®¡ç¨¿å‘¨æœŸç»Ÿè®¡åˆ†æ")
    print("=" * 100)
    
    # è·å–è¿‘ä¸€å¹´çš„é«˜å½±å“å› å­æœŸåˆŠæ–‡çŒ®
    journals_to_analyze = [
        "Nature",
        "Science", 
        "Cell",
        "Nature Reviews Molecular Cell Biology",
        "Nature Medicine",
        "Nature Biotechnology",
        "Nature Genetics",
        "Nature Immunology",
        "Nature Neuroscience",
        "Nature Cell Biology",
        "The Lancet",
        "New England Journal of Medicine",
        "JAMA",
        "Nature Communications",
        "PNAS"
    ]
    
    all_journal_stats = {}
    
    for journal in journals_to_analyze:
        print(f"\nğŸ“Š åˆ†ææœŸåˆŠ: {journal}")
        print("-" * 60)
        
        stats = analyze_single_journal(journal)
        if stats:
            all_journal_stats[journal] = stats
            print_journal_stats(journal, stats)
        else:
            print(f"âŒ æœªèƒ½è·å– {journal} çš„æ•°æ®")
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    generate_comprehensive_report(all_journal_stats)

def analyze_single_journal(journal_name: str, max_papers: int = 100) -> Optional[Dict[str, Any]]:
    """åˆ†æå•ä¸ªæœŸåˆŠçš„å®¡ç¨¿å‘¨æœŸ"""
    
    # æ„å»ºæ£€ç´¢å¼ - è¿‘ä¸€å¹´çš„æ–‡çŒ®
    search_query = f'("{journal_name}"[Journal]) AND (("2024/01/01"[Date - Create] : "2025/12/31"[Date - Create]))'
    
    try:
        # æœç´¢æ–‡çŒ®
        pmids = search_journal_papers(search_query, max_papers)
        if not pmids:
            return None
        
        print(f"   æ‰¾åˆ° {len(pmids)} ç¯‡æ–‡çŒ®")
        
        # è·å–è¯¦ç»†ä¿¡æ¯
        papers = fetch_papers_with_dates(pmids)
        if not papers:
            return None
        
        print(f"   æˆåŠŸè§£æ {len(papers)} ç¯‡æ–‡çŒ®çš„æ—¶é—´ä¿¡æ¯")
        
        # è®¡ç®—å®¡ç¨¿å‘¨æœŸ
        stats = calculate_review_cycles(papers)
        stats['total_papers'] = len(papers)
        stats['journal_name'] = journal_name
        
        return stats
        
    except Exception as e:
        print(f"   âŒ åˆ†æå¤±è´¥: {e}")
        return None

def search_journal_papers(query: str, max_results: int) -> List[str]:
    """æœç´¢æœŸåˆŠæ–‡çŒ®"""
    try:
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        params = {
            'db': 'pubmed',
            'term': query,
            'retmax': max_results,
            'retmode': 'xml',
            'tool': 'NNScholar',
            'email': 'test@nnscholar.com'
        }
        
        response = requests.get(base_url, params=params, timeout=30)
        response.raise_for_status()
        
        root = ET.fromstring(response.content)
        pmids = [id_elem.text for id_elem in root.findall('.//Id')]
        
        return pmids
        
    except Exception as e:
        print(f"   æœç´¢å¤±è´¥: {e}")
        return []

def fetch_papers_with_dates(pmids: List[str]) -> List[Dict[str, Any]]:
    """è·å–æ–‡çŒ®çš„è¯¦ç»†æ—¶é—´ä¿¡æ¯"""
    try:
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
        params = {
            'db': 'pubmed',
            'id': ','.join(pmids),
            'retmode': 'xml',
            'rettype': 'abstract',
            'tool': 'NNScholar',
            'email': 'test@nnscholar.com'
        }
        
        response = requests.get(base_url, params=params, timeout=120)
        response.raise_for_status()
        
        return parse_papers_dates(response.content)
        
    except Exception as e:
        print(f"   è·å–è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
        return []

def parse_papers_dates(xml_content: bytes) -> List[Dict[str, Any]]:
    """è§£ææ–‡çŒ®çš„æ—¶é—´ä¿¡æ¯"""
    papers = []
    
    try:
        root = ET.fromstring(xml_content)
        
        for article_elem in root.findall('.//PubmedArticle'):
            paper_dates = extract_paper_dates(article_elem)
            if paper_dates:
                papers.append(paper_dates)
                
    except ET.ParseError as e:
        print(f"   XMLè§£æå¤±è´¥: {e}")
    
    return papers

def extract_paper_dates(article_elem) -> Optional[Dict[str, Any]]:
    """æå–å•ç¯‡æ–‡çŒ®çš„æ—¶é—´ä¿¡æ¯"""
    try:
        paper = {}
        
        # åŸºæœ¬ä¿¡æ¯
        medline_citation = article_elem.find('.//MedlineCitation')
        if medline_citation is not None:
            paper['pmid'] = get_text(medline_citation.find('.//PMID'))
        
        # æ–‡ç« æ ‡é¢˜
        article = article_elem.find('.//Article')
        if article is not None:
            paper['title'] = get_text(article.find('.//ArticleTitle'))[:100] + "..."
        
        # æå–å„ç§æ—¥æœŸ
        dates = {}
        
        # ä»PubmedData/Historyä¸­æå–æ—¥æœŸ
        pubmed_data = article_elem.find('.//PubmedData')
        if pubmed_data is not None:
            history = pubmed_data.find('.//History')
            if history is not None:
                for pub_date in history.findall('.//PubMedPubDate'):
                    status = pub_date.get('PubStatus', '')
                    date_obj = parse_date_element(pub_date)
                    if date_obj:
                        dates[status] = date_obj
        
        # ä»æœŸåˆŠå‘è¡¨æ—¥æœŸä¸­æå–
        if article is not None:
            journal = article.find('.//Journal')
            if journal is not None:
                journal_issue = journal.find('.//JournalIssue')
                if journal_issue is not None:
                    pub_date = journal_issue.find('.//PubDate')
                    if pub_date is not None:
                        date_obj = parse_date_element(pub_date)
                        if date_obj:
                            dates['published'] = date_obj
        
        paper['dates'] = dates
        
        # åªè¿”å›æœ‰è¶³å¤Ÿæ—¥æœŸä¿¡æ¯çš„æ–‡çŒ®
        if len(dates) >= 2:
            return paper
        
        return None
        
    except Exception as e:
        return None

def parse_date_element(date_elem) -> Optional[datetime]:
    """è§£ææ—¥æœŸå…ƒç´ """
    try:
        year = get_text(date_elem.find('.//Year'))
        month = get_text(date_elem.find('.//Month'))
        day = get_text(date_elem.find('.//Day'))
        
        if not year:
            return None
        
        # å¤„ç†æœˆä»½
        if month:
            try:
                month_num = int(month)
            except ValueError:
                # å¤„ç†æœˆä»½åç§°
                month_names = {
                    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
                    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12
                }
                month_num = month_names.get(month[:3], 1)
        else:
            month_num = 1
        
        day_num = int(day) if day else 1
        
        return datetime(int(year), month_num, day_num)
        
    except (ValueError, TypeError):
        return None

def calculate_review_cycles(papers: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è®¡ç®—å®¡ç¨¿å‘¨æœŸç»Ÿè®¡"""
    
    review_cycles = []  # ä»æ¥æ”¶åˆ°æ¥å—çš„å¤©æ•°
    publication_cycles = []  # ä»æ¥å—åˆ°å‘è¡¨çš„å¤©æ•°
    total_cycles = []  # ä»æ¥æ”¶åˆ°å‘è¡¨çš„æ€»å¤©æ•°
    
    date_stats = {
        'received_count': 0,
        'accepted_count': 0,
        'published_count': 0,
        'complete_cycle_count': 0
    }
    
    for paper in papers:
        dates = paper.get('dates', {})
        
        # ç»Ÿè®¡å„ç§æ—¥æœŸçš„å¯ç”¨æ€§
        if 'received' in dates:
            date_stats['received_count'] += 1
        if 'accepted' in dates:
            date_stats['accepted_count'] += 1
        if 'published' in dates or 'pubmed' in dates:
            date_stats['published_count'] += 1
        
        # è®¡ç®—å®¡ç¨¿å‘¨æœŸ (received -> accepted)
        if 'received' in dates and 'accepted' in dates:
            days = (dates['accepted'] - dates['received']).days
            if 0 <= days <= 365:  # åˆç†èŒƒå›´å†…
                review_cycles.append(days)
        
        # è®¡ç®—å‘è¡¨å‘¨æœŸ (accepted -> published)
        if 'accepted' in dates and ('published' in dates or 'pubmed' in dates):
            pub_date = dates.get('published') or dates.get('pubmed')
            days = (pub_date - dates['accepted']).days
            if 0 <= days <= 365:
                publication_cycles.append(days)
        
        # è®¡ç®—æ€»å‘¨æœŸ (received -> published)
        if 'received' in dates and ('published' in dates or 'pubmed' in dates):
            pub_date = dates.get('published') or dates.get('pubmed')
            days = (pub_date - dates['received']).days
            if 0 <= days <= 730:  # 2å¹´å†…åˆç†
                total_cycles.append(days)
                date_stats['complete_cycle_count'] += 1
    
    # è®¡ç®—ç»Ÿè®¡å€¼
    stats = {
        'date_availability': date_stats,
        'review_cycle': calculate_cycle_stats(review_cycles, "å®¡ç¨¿å‘¨æœŸ"),
        'publication_cycle': calculate_cycle_stats(publication_cycles, "å‘è¡¨å‘¨æœŸ"),
        'total_cycle': calculate_cycle_stats(total_cycles, "æ€»å‘¨æœŸ")
    }
    
    return stats

def calculate_cycle_stats(cycles: List[int], cycle_name: str) -> Dict[str, Any]:
    """è®¡ç®—å‘¨æœŸç»Ÿè®¡å€¼"""
    if not cycles:
        return {
            'count': 0,
            'mean_days': 0,
            'median_days': 0,
            'min_days': 0,
            'max_days': 0,
            'std_days': 0
        }
    
    return {
        'count': len(cycles),
        'mean_days': round(statistics.mean(cycles), 1),
        'median_days': round(statistics.median(cycles), 1),
        'min_days': min(cycles),
        'max_days': max(cycles),
        'std_days': round(statistics.stdev(cycles) if len(cycles) > 1 else 0, 1)
    }

def print_journal_stats(journal_name: str, stats: Dict[str, Any]):
    """æ‰“å°æœŸåˆŠç»Ÿè®¡ç»“æœ"""
    print(f"   ğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
    print(f"      æ€»æ–‡çŒ®æ•°: {stats['total_papers']}")
    
    date_avail = stats['date_availability']
    print(f"      æ—¥æœŸå¯ç”¨æ€§:")
    print(f"        - æœ‰æ¥æ”¶æ—¥æœŸ: {date_avail['received_count']} ç¯‡")
    print(f"        - æœ‰æ¥å—æ—¥æœŸ: {date_avail['accepted_count']} ç¯‡")
    print(f"        - æœ‰å‘è¡¨æ—¥æœŸ: {date_avail['published_count']} ç¯‡")
    print(f"        - å®Œæ•´å‘¨æœŸ: {date_avail['complete_cycle_count']} ç¯‡")
    
    # å®¡ç¨¿å‘¨æœŸ
    review = stats['review_cycle']
    if review['count'] > 0:
        print(f"      å®¡ç¨¿å‘¨æœŸ (æ¥æ”¶â†’æ¥å—): {review['count']} ç¯‡æ ·æœ¬")
        print(f"        - å¹³å‡: {review['mean_days']} å¤©")
        print(f"        - ä¸­ä½æ•°: {review['median_days']} å¤©")
        print(f"        - èŒƒå›´: {review['min_days']}-{review['max_days']} å¤©")
    
    # æ€»å‘¨æœŸ
    total = stats['total_cycle']
    if total['count'] > 0:
        print(f"      æ€»å‘¨æœŸ (æ¥æ”¶â†’å‘è¡¨): {total['count']} ç¯‡æ ·æœ¬")
        print(f"        - å¹³å‡: {total['mean_days']} å¤©")
        print(f"        - ä¸­ä½æ•°: {total['median_days']} å¤©")

def generate_comprehensive_report(all_stats: Dict[str, Dict[str, Any]]):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    print(f"\n" + "=" * 100)
    print("ğŸ“Š å„æœŸåˆŠå®¡ç¨¿å‘¨æœŸç»¼åˆå¯¹æ¯”")
    print("=" * 100)
    
    # æŒ‰å®¡ç¨¿å‘¨æœŸæ’åº
    journals_with_cycles = []
    for journal, stats in all_stats.items():
        review_cycle = stats['review_cycle']
        if review_cycle['count'] > 0:
            journals_with_cycles.append((journal, review_cycle['mean_days'], stats))
    
    journals_with_cycles.sort(key=lambda x: x[1])  # æŒ‰å¹³å‡å®¡ç¨¿å‘¨æœŸæ’åº
    
    print(f"{'æœŸåˆŠåç§°':<35} {'æ ·æœ¬æ•°':<8} {'å¹³å‡å®¡ç¨¿å‘¨æœŸ':<12} {'ä¸­ä½æ•°':<8} {'èŒƒå›´':<15}")
    print("-" * 85)
    
    for journal, mean_days, stats in journals_with_cycles:
        review = stats['review_cycle']
        range_str = f"{review['min_days']}-{review['max_days']}"
        print(f"{journal:<35} {review['count']:<8} {mean_days:<12} {review['median_days']:<8} {range_str:<15}")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_data = {
        'analysis_time': datetime.now().isoformat(),
        'summary': {
            'total_journals_analyzed': len(all_stats),
            'journals_with_review_data': len(journals_with_cycles)
        },
        'journal_stats': all_stats
    }
    
    filename = f"journal_review_cycles_{timestamp}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")

def get_text(element) -> str:
    """å®‰å…¨è·å–å…ƒç´ æ–‡æœ¬"""
    if element is None:
        return ''
    return ''.join(element.itertext()).strip()

if __name__ == "__main__":
    analyze_journal_review_cycles()
