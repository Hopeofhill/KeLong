#!/usr/bin/env python3
"""
å¤šçº¿ç¨‹å¹¶å‘åˆ†ææœŸåˆŠå®¡ç¨¿å‘¨æœŸ
"""

import requests
import xml.etree.ElementTree as ET
import json
import csv
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Any, Optional
import statistics
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(threadName)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ConcurrentJournalAnalyzer:
    """å¹¶å‘æœŸåˆŠåˆ†æå™¨"""
    
    def __init__(self, max_workers: int = 8, rate_limit_delay: float = 0.2):
        self.max_workers = max_workers
        self.rate_limit_delay = rate_limit_delay
        self.results_queue = queue.Queue()
        self.progress_lock = threading.Lock()
        self.completed_count = 0
        self.total_journals = 0
        
    def analyze_all_journals_concurrent(self):
        """å¹¶å‘åˆ†ææ‰€æœ‰æœŸåˆŠ"""
        
        print("=" * 100)
        print("å¤šçº¿ç¨‹å¹¶å‘æœŸåˆŠå®¡ç¨¿å‘¨æœŸåˆ†æ")
        print("=" * 100)
        
        # è·å–æœŸåˆŠåˆ—è¡¨
        journals_list = self.get_journals_list()
        self.total_journals = len(journals_list)
        
        print(f"å‡†å¤‡å¹¶å‘åˆ†æ {self.total_journals} ä¸ªæœŸåˆŠ")
        print(f"å¹¶å‘çº¿ç¨‹æ•°: {self.max_workers}")
        print(f"é€Ÿç‡é™åˆ¶: {self.rate_limit_delay}ç§’/è¯·æ±‚")
        print("-" * 100)
        
        start_time = time.time()
        all_results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_journal = {
                executor.submit(self.analyze_single_journal_safe, journal): journal 
                for journal in journals_list
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_journal):
                journal = future_to_journal[future]
                try:
                    result = future.result()
                    if result:
                        all_results.append(result)
                        logger.info(f"âœ… {journal}: å¹³å‡å®¡ç¨¿å‘¨æœŸ {result['avg_review_days']:.1f} å¤©")
                    else:
                        logger.warning(f"âŒ {journal}: åˆ†æå¤±è´¥æˆ–æ— æ•°æ®")
                        
                except Exception as e:
                    logger.error(f"âŒ {journal}: åˆ†æå‡ºé”™ - {e}")
                
                # æ›´æ–°è¿›åº¦
                with self.progress_lock:
                    self.completed_count += 1
                    progress = (self.completed_count / self.total_journals) * 100
                    print(f"\rè¿›åº¦: {self.completed_count}/{self.total_journals} ({progress:.1f}%)", end="", flush=True)
        
        print()  # æ¢è¡Œ
        end_time = time.time()
        
        print(f"\n" + "=" * 100)
        print(f"å¹¶å‘åˆ†æå®Œæˆï¼")
        print(f"æˆåŠŸåˆ†æ: {len(all_results)}/{self.total_journals} ä¸ªæœŸåˆŠ")
        print(f"æ€»è€—æ—¶: {end_time - start_time:.1f} ç§’")
        print(f"å¹³å‡æ¯æœŸåˆŠ: {(end_time - start_time) / self.total_journals:.1f} ç§’")
        print("=" * 100)
        
        # ä¿å­˜ç»“æœ
        self.save_concurrent_results(all_results)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_concurrent_report(all_results)
        
        return all_results

    def analyze_single_journal_safe(self, journal_name: str) -> Optional[Dict[str, Any]]:
        """çº¿ç¨‹å®‰å…¨çš„å•æœŸåˆŠåˆ†æ"""
        try:
            # æ·»åŠ é€Ÿç‡é™åˆ¶
            time.sleep(self.rate_limit_delay)
            
            return self.analyze_single_journal(journal_name)
            
        except Exception as e:
            logger.error(f"æœŸåˆŠ {journal_name} åˆ†æå¼‚å¸¸: {e}")
            return None

    def analyze_single_journal(self, journal_name: str, max_papers: int = 200) -> Optional[Dict[str, Any]]:
        """åˆ†æå•ä¸ªæœŸåˆŠçš„å®¡ç¨¿å‘¨æœŸ"""
        
        # æ„å»ºæ£€ç´¢å¼
        search_query = f'("{journal_name}"[Journal]) AND (("2024/01/01"[Date - Create] : "2025/12/31"[Date - Create]))'
        
        try:
            # æœç´¢æ–‡çŒ®
            pmids = self.search_papers(search_query, max_papers)
            if not pmids:
                return None
            
            logger.debug(f"{journal_name}: æ‰¾åˆ° {len(pmids)} ç¯‡æ–‡çŒ®")
            
            # åˆ†æ‰¹è·å–è¯¦ç»†ä¿¡æ¯
            papers_with_dates = self.fetch_all_papers_concurrent(pmids)
            
            if not papers_with_dates:
                return None
            
            logger.debug(f"{journal_name}: è§£æ {len(papers_with_dates)} ç¯‡æ–‡çŒ®çš„æ—¶é—´ä¿¡æ¯")
            
            # è®¡ç®—å®¡ç¨¿å‘¨æœŸ
            review_cycles = self.calculate_review_cycles(papers_with_dates)
            
            if not review_cycles:
                return None
            
            # è®¡ç®—ç»Ÿè®¡å€¼
            result = {
                'journal_name': journal_name,
                'total_papers': len(papers_with_dates),
                'papers_with_review_data': len(review_cycles),
                'avg_review_days': round(statistics.mean(review_cycles), 1),
                'median_review_days': round(statistics.median(review_cycles), 1),
                'min_review_days': min(review_cycles),
                'max_review_days': max(review_cycles),
                'std_review_days': round(statistics.stdev(review_cycles) if len(review_cycles) > 1 else 0, 1),
                'data_completeness': round((len(review_cycles) / len(papers_with_dates)) * 100, 1),
                'analysis_thread': threading.current_thread().name,
                'analysis_time': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            logger.error(f"{journal_name} åˆ†æå¤±è´¥: {e}")
            return None

    def fetch_all_papers_concurrent(self, pmids: List[str], batch_size: int = 50) -> List[Dict[str, Any]]:
        """å¹¶å‘è·å–æ‰€æœ‰æ–‡çŒ®çš„è¯¦ç»†ä¿¡æ¯"""
        all_papers = []
        
        # åˆ†æ‰¹å¤„ç†
        batches = [pmids[i:i + batch_size] for i in range(0, len(pmids), batch_size)]
        
        # å¯¹äºå•ä¸ªæœŸåˆŠå†…çš„æ‰¹æ¬¡ï¼Œä½¿ç”¨è¾ƒå°çš„å¹¶å‘æ•°é¿å…è¿‡åº¦è¯·æ±‚
        max_batch_workers = min(3, len(batches))
        
        with ThreadPoolExecutor(max_workers=max_batch_workers) as executor:
            future_to_batch = {
                executor.submit(self.fetch_papers_batch, batch): batch 
                for batch in batches
            }
            
            for future in as_completed(future_to_batch):
                try:
                    batch_papers = future.result()
                    all_papers.extend(batch_papers)
                except Exception as e:
                    logger.error(f"æ‰¹æ¬¡è·å–å¤±è´¥: {e}")
                    continue
        
        return all_papers

    def search_papers(self, query: str, max_results: int) -> List[str]:
        """æœç´¢æ–‡çŒ®è·å–PMIDåˆ—è¡¨"""
        try:
            base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
            params = {
                'db': 'pubmed',
                'term': query,
                'retmax': max_results,
                'retmode': 'xml',
                'tool': 'NNScholar',
                'email': 'test@nnscholar.com'
            }
            
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            pmids = [id_elem.text for id_elem in root.findall('.//Id')]
            
            return pmids
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []

    def fetch_papers_batch(self, pmids: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡è·å–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯"""
        try:
            base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
            params = {
                'db': 'pubmed',
                'id': ','.join(pmids),
                'retmode': 'xml',
                'rettype': 'abstract',
                'tool': 'NNScholar',
                'email': 'test@nnscholar.com'
            }
            
            response = requests.get(base_url, params=params, timeout=120)
            response.raise_for_status()
            
            return self.parse_papers_xml(response.content)
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡è·å–å¤±è´¥: {e}")
            return []

    def parse_papers_xml(self, xml_content: bytes) -> List[Dict[str, Any]]:
        """è§£ææ–‡çŒ®XMLæ•°æ®"""
        papers = []
        
        try:
            root = ET.fromstring(xml_content)
            
            for article_elem in root.findall('.//PubmedArticle'):
                paper_data = self.extract_paper_dates(article_elem)
                if paper_data:
                    papers.append(paper_data)
                    
        except ET.ParseError as e:
            logger.error(f"XMLè§£æå¤±è´¥: {e}")
        
        return papers

    def extract_paper_dates(self, article_elem) -> Optional[Dict[str, Any]]:
        """æå–å•ç¯‡æ–‡çŒ®çš„æ—¶é—´ä¿¡æ¯"""
        try:
            # è·å–PMID
            medline_citation = article_elem.find('.//MedlineCitation')
            if medline_citation is None:
                return None
            
            pmid = self.get_text(medline_citation.find('.//PMID'))
            if not pmid:
                return None
            
            # æå–æ—¶é—´ä¿¡æ¯
            dates = {}
            
            # ä»PubmedData/Historyä¸­æå–å„ç§çŠ¶æ€çš„æ—¥æœŸ
            pubmed_data = article_elem.find('.//PubmedData')
            if pubmed_data is not None:
                history = pubmed_data.find('.//History')
                if history is not None:
                    for pub_date in history.findall('.//PubMedPubDate'):
                        status = pub_date.get('PubStatus', '')
                        date_obj = self.parse_date_element(pub_date)
                        if date_obj:
                            dates[status] = date_obj
            
            # åªè¿”å›æœ‰receivedå’Œacceptedæ—¥æœŸçš„æ–‡çŒ®
            if 'received' in dates and 'accepted' in dates:
                return {
                    'pmid': pmid,
                    'received_date': dates['received'],
                    'accepted_date': dates['accepted'],
                    'all_dates': dates
                }
            
            return None
            
        except Exception as e:
            logger.error(f"æå–æ–‡çŒ®æ—¥æœŸå¤±è´¥: {e}")
            return None

    def parse_date_element(self, date_elem) -> Optional[datetime]:
        """ä»XMLå…ƒç´ è§£ææ—¥æœŸ"""
        try:
            year = self.get_text(date_elem.find('.//Year'))
            month = self.get_text(date_elem.find('.//Month'))
            day = self.get_text(date_elem.find('.//Day'))
            
            if not year:
                return None
            
            # å¤„ç†æœˆä»½
            if month:
                try:
                    month_num = int(month)
                except ValueError:
                    month_names = {
                        'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
                        'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12
                    }
                    month_num = month_names.get(month[:3], 1)
            else:
                month_num = 1
            
            day_num = int(day) if day else 1
            
            return datetime(int(year), month_num, day_num)
            
        except (ValueError, TypeError):
            return None

    def calculate_review_cycles(self, papers: List[Dict[str, Any]]) -> List[int]:
        """è®¡ç®—å®¡ç¨¿å‘¨æœŸ"""
        review_cycles = []
        
        for paper in papers:
            received_date = paper.get('received_date')
            accepted_date = paper.get('accepted_date')
            
            if received_date and accepted_date:
                days = (accepted_date - received_date).days
                if 0 <= days <= 730:  # è¿‡æ»¤å¼‚å¸¸å€¼
                    review_cycles.append(days)
        
        return review_cycles

    def get_journals_list(self) -> List[str]:
        """è·å–æœŸåˆŠåˆ—è¡¨"""
        return [
            # é¡¶çº§æœŸåˆŠ
            "Nature", "Science", "Cell",
            "Nature Medicine", "Nature Biotechnology", "Nature Genetics",
            "Nature Immunology", "Nature Neuroscience", "Nature Cell Biology",
            "Nature Communications", "PNAS", "eLife",
            
            # Cellç³»åˆ—
            "Cell Metabolism", "Cell Stem Cell", "Cancer Cell",
            "Molecular Cell", "Developmental Cell", "Current Biology",
            
            # åŒ»å­¦æœŸåˆŠ
            "The Lancet", "New England Journal of Medicine", "JAMA",
            "The Lancet Oncology", "Blood", "Immunity",
            
            # å…¶ä»–é‡è¦æœŸåˆŠ
            "EMBO Journal", "EMBO Reports", "PLoS Biology",
            "Genome Research", "Genome Biology", "Nucleic Acids Research",
            "Bioinformatics", "Nature Methods", "Nature Structural & Molecular Biology"
        ]

    def save_concurrent_results(self, results: List[Dict[str, Any]]):
        """ä¿å­˜å¹¶å‘åˆ†æç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSONæ ¼å¼
        json_filename = f"concurrent_journal_analysis_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump({
                'analysis_method': 'concurrent_multithreading',
                'max_workers': self.max_workers,
                'rate_limit_delay': self.rate_limit_delay,
                'analysis_time': datetime.now().isoformat(),
                'total_journals_analyzed': len(results),
                'results': results
            }, f, ensure_ascii=False, indent=2, default=str)
        
        # CSVæ ¼å¼
        csv_filename = f"concurrent_journal_analysis_{timestamp}.csv"
        with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'æœŸåˆŠåç§°', 'æ€»æ–‡çŒ®æ•°', 'æœ‰å®¡ç¨¿æ•°æ®æ–‡çŒ®æ•°', 'å¹³å‡å®¡ç¨¿å‘¨æœŸ(å¤©)', 
                'ä¸­ä½æ•°(å¤©)', 'æœ€çŸ­(å¤©)', 'æœ€é•¿(å¤©)', 'æ ‡å‡†å·®(å¤©)', 'æ•°æ®å®Œæ•´ç‡(%)', 'åˆ†æçº¿ç¨‹'
            ])
            
            for result in results:
                writer.writerow([
                    result['journal_name'], result['total_papers'],
                    result['papers_with_review_data'], result['avg_review_days'],
                    result['median_review_days'], result['min_review_days'],
                    result['max_review_days'], result['std_review_days'],
                    result['data_completeness'], result['analysis_thread']
                ])
        
        print(f"\nâœ… å¹¶å‘åˆ†æç»“æœå·²ä¿å­˜:")
        print(f"   JSON: {json_filename}")
        print(f"   CSV: {csv_filename}")

    def generate_concurrent_report(self, results: List[Dict[str, Any]]):
        """ç”Ÿæˆå¹¶å‘åˆ†ææŠ¥å‘Š"""
        if not results:
            return
        
        # æŒ‰å¹³å‡å®¡ç¨¿å‘¨æœŸæ’åº
        sorted_results = sorted(results, key=lambda x: x['avg_review_days'])
        
        print(f"\nğŸ“Š å¹¶å‘åˆ†ææœŸåˆŠå®¡ç¨¿å‘¨æœŸæ’è¡Œæ¦œ")
        print("=" * 120)
        print(f"{'æ’å':<4} {'æœŸåˆŠåç§°':<35} {'å¹³å‡å‘¨æœŸ':<10} {'ä¸­ä½æ•°':<8} {'æ ·æœ¬æ•°':<8} {'å®Œæ•´ç‡':<8} {'åˆ†æçº¿ç¨‹':<15}")
        print("-" * 120)
        
        for i, result in enumerate(sorted_results, 1):
            print(f"{i:<4} {result['journal_name']:<35} {result['avg_review_days']:<10.1f} "
                  f"{result['median_review_days']:<8.1f} {result['papers_with_review_data']:<8} "
                  f"{result['data_completeness']:<8.1f}% {result['analysis_thread']:<15}")
        
        # æ€§èƒ½ç»Ÿè®¡
        avg_cycles = [r['avg_review_days'] for r in results]
        thread_distribution = {}
        for result in results:
            thread = result['analysis_thread']
            thread_distribution[thread] = thread_distribution.get(thread, 0) + 1
        
        print(f"\nğŸ“ˆ å¹¶å‘åˆ†æç»Ÿè®¡:")
        print(f"   æˆåŠŸåˆ†ææœŸåˆŠæ•°: {len(results)}")
        print(f"   å¹³å‡å®¡ç¨¿å‘¨æœŸ: {statistics.mean(avg_cycles):.1f} å¤©")
        print(f"   æœ€å¿«æœŸåˆŠ: {sorted_results[0]['journal_name']} ({sorted_results[0]['avg_review_days']:.1f}å¤©)")
        print(f"   æœ€æ…¢æœŸåˆŠ: {sorted_results[-1]['journal_name']} ({sorted_results[-1]['avg_review_days']:.1f}å¤©)")
        print(f"   çº¿ç¨‹åˆ†å¸ƒ: {dict(sorted(thread_distribution.items()))}")

    def get_text(self, element) -> str:
        """å®‰å…¨è·å–å…ƒç´ æ–‡æœ¬"""
        if element is None:
            return ''
        return ''.join(element.itertext()).strip()

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå¹¶å‘åˆ†æå™¨
    analyzer = ConcurrentJournalAnalyzer(
        max_workers=8,  # 8ä¸ªå¹¶å‘çº¿ç¨‹
        rate_limit_delay=0.3  # æ¯ä¸ªè¯·æ±‚é—´éš”0.3ç§’
    )
    
    # æ‰§è¡Œå¹¶å‘åˆ†æ
    results = analyzer.analyze_all_journals_concurrent()
    
    print(f"\nğŸ‰ å¹¶å‘åˆ†æå®Œæˆï¼å…±åˆ†æäº† {len(results)} ä¸ªæœŸåˆŠ")

if __name__ == "__main__":
    main()
