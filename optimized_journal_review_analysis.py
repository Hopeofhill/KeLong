#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„æœŸåˆŠå®¡ç¨¿å‘¨æœŸåˆ†æ - æ¯æœŸåˆŠé™åˆ¶50ç¯‡æ–‡çŒ®
"""

import requests
import xml.etree.ElementTree as ET
import json
import csv
from datetime import datetime
from typing import Dict, List, Any, Optional
import statistics
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(threadName)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimizedJournalAnalyzer:
    """ä¼˜åŒ–çš„æœŸåˆŠåˆ†æå™¨ - æ¯æœŸåˆŠ50ç¯‡æ–‡çŒ®æ ·æœ¬"""
    
    def __init__(self, max_workers: int = 10, sample_size: int = 50, rate_limit: float = 0.2):
        self.max_workers = max_workers
        self.sample_size = sample_size
        self.rate_limit = rate_limit
        self.progress_lock = threading.Lock()
        self.completed_count = 0
        self.total_journals = 0
        
    def analyze_journals_optimized(self):
        """ä¼˜åŒ–çš„æœŸåˆŠåˆ†ææµç¨‹"""
        
        print("=" * 100)
        print("ä¼˜åŒ–æœŸåˆŠå®¡ç¨¿å‘¨æœŸåˆ†æ (æ¯æœŸåˆŠ50ç¯‡æ–‡çŒ®æ ·æœ¬)")
        print("=" * 100)
        
        # è·å–æœŸåˆŠåˆ—è¡¨
        journals_list = self.get_comprehensive_journals_list()
        self.total_journals = len(journals_list)
        
        print(f"æœŸåˆŠæ€»æ•°: {self.total_journals}")
        print(f"æ¯æœŸåˆŠæ ·æœ¬: {self.sample_size} ç¯‡æ–‡çŒ®")
        print(f"å¹¶å‘çº¿ç¨‹: {self.max_workers}")
        print(f"é¢„è®¡æ€»æ–‡çŒ®: {self.total_journals * self.sample_size}")
        print("-" * 100)
        
        start_time = time.time()
        successful_results = []
        
        # å¹¶å‘åˆ†æ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_journal = {
                executor.submit(self.analyze_single_journal_optimized, journal): journal 
                for journal in journals_list
            }
            
            for future in as_completed(future_to_journal):
                journal = future_to_journal[future]
                try:
                    result = future.result()
                    if result:
                        successful_results.append(result)
                        self.log_progress(journal, result)
                    else:
                        logger.warning(f"âŒ {journal}: æ— æœ‰æ•ˆå®¡ç¨¿æ•°æ®")
                        
                except Exception as e:
                    logger.error(f"âŒ {journal}: åˆ†æå¼‚å¸¸ - {e}")
                
                self.update_progress()
        
        end_time = time.time()
        self.print_completion_summary(successful_results, end_time - start_time)
        
        # ä¿å­˜å’ŒæŠ¥å‘Š
        self.save_optimized_results(successful_results)
        self.generate_comprehensive_report(successful_results)
        
        return successful_results

    def analyze_single_journal_optimized(self, journal_name: str) -> Optional[Dict[str, Any]]:
        """ä¼˜åŒ–çš„å•æœŸåˆŠåˆ†æ - é™åˆ¶50ç¯‡æ–‡çŒ®"""
        
        # æ·»åŠ é€Ÿç‡é™åˆ¶
        time.sleep(self.rate_limit)
        
        # æ„å»ºæ£€ç´¢å¼ - è¿‘ä¸€å¹´æ–‡çŒ®
        search_query = f'("{journal_name}"[Journal]) AND (("2024/01/01"[Date - Create] : "2025/12/31"[Date - Create]))'
        
        try:
            # æ­¥éª¤1: æœç´¢è·å–PMID (é™åˆ¶50ç¯‡)
            pmids = self.search_papers_limited(search_query, self.sample_size)
            if not pmids:
                return None
            
            # æ­¥éª¤2: è·å–è¯¦ç»†ä¿¡æ¯
            papers_with_dates = self.fetch_papers_with_review_dates(pmids)
            if not papers_with_dates:
                return None
            
            # æ­¥éª¤3: è®¡ç®—å®¡ç¨¿å‘¨æœŸ
            review_data = self.calculate_comprehensive_review_metrics(papers_with_dates)
            if not review_data['review_cycles']:
                return None
            
            # æ­¥éª¤4: æ„å»ºç»“æœ
            result = {
                'journal_name': journal_name,
                'sample_size': len(pmids),
                'papers_analyzed': len(papers_with_dates),
                'papers_with_review_data': len(review_data['review_cycles']),
                'data_completeness_rate': round((len(review_data['review_cycles']) / len(papers_with_dates)) * 100, 1),
                
                # å®¡ç¨¿å‘¨æœŸç»Ÿè®¡
                'avg_review_days': round(statistics.mean(review_data['review_cycles']), 1),
                'median_review_days': round(statistics.median(review_data['review_cycles']), 1),
                'min_review_days': min(review_data['review_cycles']),
                'max_review_days': max(review_data['review_cycles']),
                'std_review_days': round(statistics.stdev(review_data['review_cycles']) if len(review_data['review_cycles']) > 1 else 0, 1),
                
                # å‘è¡¨å‘¨æœŸç»Ÿè®¡
                'publication_metrics': review_data['publication_metrics'],
                
                # å…ƒæ•°æ®
                'analysis_thread': threading.current_thread().name,
                'analysis_timestamp': datetime.now().isoformat(),
                'sample_pmids': pmids[:10]  # ä¿å­˜å‰10ä¸ªPMIDä½œä¸ºæ ·æœ¬
            }
            
            return result
            
        except Exception as e:
            logger.error(f"{journal_name} åˆ†æå¤±è´¥: {e}")
            return None

    def search_papers_limited(self, query: str, limit: int) -> List[str]:
        """æœç´¢æ–‡çŒ® - é™åˆ¶æ•°é‡"""
        try:
            base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
            params = {
                'db': 'pubmed',
                'term': query,
                'retmax': limit,
                'retmode': 'xml',
                'sort': 'relevance',  # æŒ‰ç›¸å…³æ€§æ’åºè·å–æœ€ç›¸å…³çš„æ–‡çŒ®
                'tool': 'NNScholar',
                'email': 'test@nnscholar.com'
            }
            
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            pmids = [id_elem.text for id_elem in root.findall('.//Id')]
            
            return pmids[:limit]  # ç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []

    def fetch_papers_with_review_dates(self, pmids: List[str]) -> List[Dict[str, Any]]:
        """è·å–åŒ…å«å®¡ç¨¿æ—¥æœŸçš„æ–‡çŒ®è¯¦ç»†ä¿¡æ¯"""
        try:
            base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
            params = {
                'db': 'pubmed',
                'id': ','.join(pmids),
                'retmode': 'xml',
                'rettype': 'abstract',
                'tool': 'NNScholar',
                'email': 'test@nnscholar.com'
            }
            
            response = requests.get(base_url, params=params, timeout=120)
            response.raise_for_status()
            
            return self.parse_papers_for_review_analysis(response.content)
            
        except Exception as e:
            logger.error(f"è·å–æ–‡çŒ®è¯¦æƒ…å¤±è´¥: {e}")
            return []

    def parse_papers_for_review_analysis(self, xml_content: bytes) -> List[Dict[str, Any]]:
        """è§£ææ–‡çŒ®XML - ä¸“æ³¨äºå®¡ç¨¿å‘¨æœŸåˆ†æ"""
        papers = []
        
        try:
            root = ET.fromstring(xml_content)
            
            for article_elem in root.findall('.//PubmedArticle'):
                paper_data = self.extract_comprehensive_dates(article_elem)
                if paper_data:
                    papers.append(paper_data)
                    
        except ET.ParseError as e:
            logger.error(f"XMLè§£æå¤±è´¥: {e}")
        
        return papers

    def extract_comprehensive_dates(self, article_elem) -> Optional[Dict[str, Any]]:
        """æå–æ–‡çŒ®çš„å®Œæ•´æ—¶é—´ä¿¡æ¯"""
        try:
            # åŸºæœ¬ä¿¡æ¯
            medline_citation = article_elem.find('.//MedlineCitation')
            if medline_citation is None:
                return None
            
            pmid = self.get_text(medline_citation.find('.//PMID'))
            if not pmid:
                return None
            
            # æå–æ‰€æœ‰æ—¶é—´ä¿¡æ¯
            all_dates = {}
            
            # ä»PubmedData/Historyæå–
            pubmed_data = article_elem.find('.//PubmedData')
            if pubmed_data is not None:
                history = pubmed_data.find('.//History')
                if history is not None:
                    for pub_date in history.findall('.//PubMedPubDate'):
                        status = pub_date.get('PubStatus', '')
                        date_obj = self.parse_date_with_precision(pub_date)
                        if date_obj:
                            all_dates[status] = date_obj
                
                # å‘è¡¨çŠ¶æ€
                pub_status = pubmed_data.find('.//PublicationStatus')
                publication_status = self.get_text(pub_status) if pub_status is not None else ''
            
            # ä»æœŸåˆŠä¿¡æ¯æå–å‘è¡¨æ—¥æœŸ
            article = article_elem.find('.//Article')
            if article is not None:
                journal = article.find('.//Journal')
                if journal is not None:
                    journal_issue = journal.find('.//JournalIssue')
                    if journal_issue is not None:
                        pub_date = journal_issue.find('.//PubDate')
                        if pub_date is not None:
                            journal_date = self.parse_date_with_precision(pub_date)
                            if journal_date:
                                all_dates['journal_published'] = journal_date
            
            # åªè¿”å›æœ‰è¶³å¤Ÿæ—¶é—´ä¿¡æ¯çš„æ–‡çŒ®
            if len(all_dates) >= 2:
                return {
                    'pmid': pmid,
                    'all_dates': all_dates,
                    'publication_status': publication_status,
                    'has_received': 'received' in all_dates,
                    'has_accepted': 'accepted' in all_dates,
                    'has_published': any(key in all_dates for key in ['pubmed', 'published', 'journal_published'])
                }
            
            return None
            
        except Exception as e:
            logger.error(f"æå–æ—¥æœŸä¿¡æ¯å¤±è´¥: {e}")
            return None

    def parse_date_with_precision(self, date_elem) -> Optional[datetime]:
        """é«˜ç²¾åº¦æ—¥æœŸè§£æ"""
        try:
            year = self.get_text(date_elem.find('.//Year'))
            month = self.get_text(date_elem.find('.//Month'))
            day = self.get_text(date_elem.find('.//Day'))
            hour = self.get_text(date_elem.find('.//Hour'))
            minute = self.get_text(date_elem.find('.//Minute'))
            
            if not year:
                return None
            
            # å¤„ç†æœˆä»½
            if month:
                try:
                    month_num = int(month)
                except ValueError:
                    month_names = {
                        'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
                        'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12
                    }
                    month_num = month_names.get(month[:3], 1)
            else:
                month_num = 1
            
            day_num = int(day) if day else 1
            hour_num = int(hour) if hour else 0
            minute_num = int(minute) if minute else 0
            
            return datetime(int(year), month_num, day_num, hour_num, minute_num)
            
        except (ValueError, TypeError):
            return None

    def calculate_comprehensive_review_metrics(self, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆå®¡ç¨¿æŒ‡æ ‡"""
        review_cycles = []  # å®¡ç¨¿å‘¨æœŸ (received -> accepted)
        publication_delays = []  # å‘è¡¨å»¶è¿Ÿ (accepted -> published)
        total_cycles = []  # æ€»å‘¨æœŸ (received -> published)
        
        date_availability = {
            'received_count': 0,
            'accepted_count': 0,
            'published_count': 0,
            'complete_cycle_count': 0
        }
        
        for paper in papers:
            dates = paper.get('all_dates', {})
            
            # ç»Ÿè®¡æ—¥æœŸå¯ç”¨æ€§
            if 'received' in dates:
                date_availability['received_count'] += 1
            if 'accepted' in dates:
                date_availability['accepted_count'] += 1
            if any(key in dates for key in ['pubmed', 'published', 'journal_published']):
                date_availability['published_count'] += 1
            
            # è®¡ç®—å®¡ç¨¿å‘¨æœŸ (received -> accepted)
            if 'received' in dates and 'accepted' in dates:
                days = (dates['accepted'] - dates['received']).days
                if 0 <= days <= 730:  # 2å¹´å†…åˆç†
                    review_cycles.append(days)
            
            # è®¡ç®—å‘è¡¨å»¶è¿Ÿ (accepted -> published)
            if 'accepted' in dates:
                pub_date = dates.get('pubmed') or dates.get('published') or dates.get('journal_published')
                if pub_date:
                    days = (pub_date - dates['accepted']).days
                    if 0 <= days <= 365:  # 1å¹´å†…åˆç†
                        publication_delays.append(days)
            
            # è®¡ç®—æ€»å‘¨æœŸ (received -> published)
            if 'received' in dates:
                pub_date = dates.get('pubmed') or dates.get('published') or dates.get('journal_published')
                if pub_date:
                    days = (pub_date - dates['received']).days
                    if 0 <= days <= 1095:  # 3å¹´å†…åˆç†
                        total_cycles.append(days)
                        date_availability['complete_cycle_count'] += 1
        
        # è®¡ç®—å‘è¡¨å‘¨æœŸç»Ÿè®¡
        publication_metrics = {}
        if publication_delays:
            publication_metrics = {
                'avg_publication_delay': round(statistics.mean(publication_delays), 1),
                'median_publication_delay': round(statistics.median(publication_delays), 1),
                'publication_delay_samples': len(publication_delays)
            }
        
        if total_cycles:
            publication_metrics.update({
                'avg_total_cycle': round(statistics.mean(total_cycles), 1),
                'median_total_cycle': round(statistics.median(total_cycles), 1),
                'total_cycle_samples': len(total_cycles)
            })
        
        return {
            'review_cycles': review_cycles,
            'publication_metrics': publication_metrics,
            'date_availability': date_availability
        }

    def get_comprehensive_journals_list(self) -> List[str]:
        """è·å–ç»¼åˆæœŸåˆŠåˆ—è¡¨"""
        return [
            # é¡¶çº§ç»¼åˆæœŸåˆŠ
            "Nature", "Science", "Cell", "PNAS",
            
            # Natureç³»åˆ—
            "Nature Medicine", "Nature Biotechnology", "Nature Genetics",
            "Nature Immunology", "Nature Neuroscience", "Nature Cell Biology",
            "Nature Communications", "Nature Methods", "Nature Structural & Molecular Biology",
            "Nature Chemical Biology", "Nature Reviews Molecular Cell Biology",
            "Nature Reviews Drug Discovery", "Nature Reviews Cancer",
            "Nature Reviews Immunology", "Nature Reviews Genetics",
            
            # Cellç³»åˆ—
            "Cell Metabolism", "Cell Stem Cell", "Cancer Cell",
            "Molecular Cell", "Developmental Cell", "Current Biology",
            "Cell Reports", "Cell Host & Microbe", "Cell Chemical Biology",
            
            # åŒ»å­¦æœŸåˆŠ
            "The Lancet", "New England Journal of Medicine", "JAMA",
            "BMJ", "Annals of Internal Medicine", "The Lancet Oncology",
            "The Lancet Neurology", "Blood", "Circulation",
            
            # ç”Ÿç‰©åŒ»å­¦æœŸåˆŠ
            "eLife", "EMBO Journal", "EMBO Reports", "PLoS Biology",
            "Journal of Clinical Investigation", "Immunity", "Neuron",
            "Cancer Research", "Journal of Experimental Medicine",
            "Genes & Development", "Molecular Biology of the Cell",
            
            # ç”Ÿç‰©ä¿¡æ¯å­¦å’ŒåŸºå› ç»„å­¦
            "Genome Research", "Genome Biology", "Nucleic Acids Research",
            "Bioinformatics", "Nature Genetics", "Genome Medicine",
            
            # å…¶ä»–é‡è¦æœŸåˆŠ
            "Science Translational Medicine", "Science Immunology",
            "Proceedings of the Royal Society B", "Journal of Cell Biology",
            "Plant Cell", "Development", "EMBO Molecular Medicine"
        ]

    def log_progress(self, journal: str, result: Dict[str, Any]):
        """è®°å½•è¿›åº¦"""
        logger.info(f"âœ… {journal}: å®¡ç¨¿å‘¨æœŸ {result['avg_review_days']:.1f}å¤© "
                   f"(æ ·æœ¬: {result['papers_with_review_data']}/{result['sample_size']})")

    def update_progress(self):
        """æ›´æ–°è¿›åº¦æ˜¾ç¤º"""
        with self.progress_lock:
            self.completed_count += 1
            progress = (self.completed_count / self.total_journals) * 100
            print(f"\rè¿›åº¦: {self.completed_count}/{self.total_journals} ({progress:.1f}%)", end="", flush=True)

    def print_completion_summary(self, results: List[Dict[str, Any]], elapsed_time: float):
        """æ‰“å°å®Œæˆæ‘˜è¦"""
        print()  # æ¢è¡Œ
        print(f"\n" + "=" * 100)
        print(f"ğŸ‰ ä¼˜åŒ–åˆ†æå®Œæˆï¼")
        print(f"æˆåŠŸåˆ†æ: {len(results)}/{self.total_journals} ä¸ªæœŸåˆŠ")
        print(f"æ€»è€—æ—¶: {elapsed_time:.1f} ç§’")
        print(f"å¹³å‡æ¯æœŸåˆŠ: {elapsed_time / self.total_journals:.1f} ç§’")
        print(f"æ€»æ–‡çŒ®åˆ†æ: {sum(r['papers_analyzed'] for r in results)} ç¯‡")
        print("=" * 100)

    def save_optimized_results(self, results: List[Dict[str, Any]]):
        """ä¿å­˜ä¼˜åŒ–åˆ†æç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSONæ ¼å¼ - è¯¦ç»†æ•°æ®
        json_filename = f"optimized_journal_analysis_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump({
                'analysis_method': 'optimized_sampling',
                'sample_size_per_journal': self.sample_size,
                'max_workers': self.max_workers,
                'analysis_time': datetime.now().isoformat(),
                'total_journals_analyzed': len(results),
                'total_papers_analyzed': sum(r['papers_analyzed'] for r in results),
                'results': results
            }, f, ensure_ascii=False, indent=2, default=str)
        
        # CSVæ ¼å¼ - æ±‡æ€»æ•°æ®
        csv_filename = f"optimized_journal_analysis_{timestamp}.csv"
        with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'æœŸåˆŠåç§°', 'æ ·æœ¬å¤§å°', 'åˆ†ææ–‡çŒ®æ•°', 'æœ‰å®¡ç¨¿æ•°æ®', 'æ•°æ®å®Œæ•´ç‡(%)',
                'å¹³å‡å®¡ç¨¿å‘¨æœŸ(å¤©)', 'ä¸­ä½æ•°(å¤©)', 'æœ€çŸ­(å¤©)', 'æœ€é•¿(å¤©)', 'æ ‡å‡†å·®(å¤©)',
                'å¹³å‡å‘è¡¨å»¶è¿Ÿ(å¤©)', 'å¹³å‡æ€»å‘¨æœŸ(å¤©)', 'åˆ†æçº¿ç¨‹'
            ])
            
            for result in results:
                pub_metrics = result.get('publication_metrics', {})
                writer.writerow([
                    result['journal_name'], result['sample_size'], result['papers_analyzed'],
                    result['papers_with_review_data'], result['data_completeness_rate'],
                    result['avg_review_days'], result['median_review_days'],
                    result['min_review_days'], result['max_review_days'], result['std_review_days'],
                    pub_metrics.get('avg_publication_delay', 'N/A'),
                    pub_metrics.get('avg_total_cycle', 'N/A'),
                    result['analysis_thread']
                ])
        
        print(f"\nâœ… ä¼˜åŒ–åˆ†æç»“æœå·²ä¿å­˜:")
        print(f"   è¯¦ç»†æ•°æ® (JSON): {json_filename}")
        print(f"   æ±‡æ€»æ•°æ® (CSV): {csv_filename}")

    def generate_comprehensive_report(self, results: List[Dict[str, Any]]):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        if not results:
            return
        
        # æŒ‰å¹³å‡å®¡ç¨¿å‘¨æœŸæ’åº
        sorted_results = sorted(results, key=lambda x: x['avg_review_days'])
        
        print(f"\nğŸ“Š æœŸåˆŠå®¡ç¨¿å‘¨æœŸæ’è¡Œæ¦œ (åŸºäº50ç¯‡æ–‡çŒ®æ ·æœ¬)")
        print("=" * 130)
        print(f"{'æ’å':<4} {'æœŸåˆŠåç§°':<40} {'å®¡ç¨¿å‘¨æœŸ':<10} {'ä¸­ä½æ•°':<8} {'æ ·æœ¬':<6} {'å®Œæ•´ç‡':<8} {'å‘è¡¨å»¶è¿Ÿ':<10}")
        print("-" * 130)
        
        for i, result in enumerate(sorted_results, 1):
            pub_delay = result.get('publication_metrics', {}).get('avg_publication_delay', 'N/A')
            pub_delay_str = f"{pub_delay:.1f}" if isinstance(pub_delay, (int, float)) else str(pub_delay)
            
            print(f"{i:<4} {result['journal_name']:<40} {result['avg_review_days']:<10.1f} "
                  f"{result['median_review_days']:<8.1f} {result['papers_with_review_data']:<6} "
                  f"{result['data_completeness_rate']:<8.1f}% {pub_delay_str:<10}")
        
        # ç»Ÿè®¡æ‘˜è¦
        avg_cycles = [r['avg_review_days'] for r in results]
        total_papers = sum(r['papers_analyzed'] for r in results)
        total_with_data = sum(r['papers_with_review_data'] for r in results)
        
        print(f"\nğŸ“ˆ åˆ†ææ‘˜è¦:")
        print(f"   æˆåŠŸåˆ†ææœŸåˆŠ: {len(results)}")
        print(f"   æ€»åˆ†ææ–‡çŒ®: {total_papers} ç¯‡")
        print(f"   æœ‰æ•ˆå®¡ç¨¿æ•°æ®: {total_with_data} ç¯‡ ({total_with_data/total_papers*100:.1f}%)")
        print(f"   å¹³å‡å®¡ç¨¿å‘¨æœŸ: {statistics.mean(avg_cycles):.1f} å¤©")
        print(f"   ä¸­ä½æ•°å®¡ç¨¿å‘¨æœŸ: {statistics.median(avg_cycles):.1f} å¤©")
        print(f"   æœ€å¿«æœŸåˆŠ: {sorted_results[0]['journal_name']} ({sorted_results[0]['avg_review_days']:.1f}å¤©)")
        print(f"   æœ€æ…¢æœŸåˆŠ: {sorted_results[-1]['journal_name']} ({sorted_results[-1]['avg_review_days']:.1f}å¤©)")

    def get_text(self, element) -> str:
        """å®‰å…¨è·å–å…ƒç´ æ–‡æœ¬"""
        if element is None:
            return ''
        return ''.join(element.itertext()).strip()

def main():
    """ä¸»å‡½æ•°"""
    analyzer = OptimizedJournalAnalyzer(
        max_workers=10,      # 10ä¸ªå¹¶å‘çº¿ç¨‹
        sample_size=50,      # æ¯æœŸåˆŠ50ç¯‡æ–‡çŒ®
        rate_limit=0.2       # æ¯è¯·æ±‚0.2ç§’é—´éš”
    )
    
    results = analyzer.analyze_journals_optimized()
    print(f"\nğŸ‰ åˆ†æå®Œæˆï¼å…±åˆ†æ {len(results)} ä¸ªæœŸåˆŠ")

if __name__ == "__main__":
    main()
