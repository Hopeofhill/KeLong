#!/usr/bin/env python3
"""
è®¡ç®—æ‰€æœ‰æœŸåˆŠä»æ”¶åˆ°ç¨¿ä»¶åˆ°æ¥æ”¶çš„å¹³å‡å‘¨æœŸ
"""

import requests
import xml.etree.ElementTree as ET
import json
import csv
from datetime import datetime, timedelta
from collections import defaultdict
from typing import Dict, List, Any, Optional
import statistics
import time

def calculate_all_journals_review_cycles():
    """è®¡ç®—æ‰€æœ‰æœŸåˆŠçš„å®¡ç¨¿å‘¨æœŸ"""
    
    print("=" * 100)
    print("æ‰€æœ‰æœŸåˆŠå®¡ç¨¿å‘¨æœŸç»Ÿè®¡åˆ†æ")
    print("=" * 100)
    
    # è·å–é«˜å½±å“å› å­æœŸåˆŠåˆ—è¡¨
    journals_list = get_top_journals_list()
    
    print(f"å‡†å¤‡åˆ†æ {len(journals_list)} ä¸ªæœŸåˆŠçš„å®¡ç¨¿å‘¨æœŸ")
    print("-" * 100)
    
    all_results = []
    successful_count = 0
    
    for i, journal in enumerate(journals_list, 1):
        print(f"\n[{i}/{len(journals_list)}] åˆ†ææœŸåˆŠ: {journal}")
        print("-" * 60)
        
        try:
            result = analyze_journal_review_cycle(journal)
            if result:
                all_results.append(result)
                successful_count += 1
                print(f"âœ… æˆåŠŸåˆ†æï¼Œå¹³å‡å®¡ç¨¿å‘¨æœŸ: {result['avg_review_days']:.1f} å¤©")
            else:
                print(f"âŒ åˆ†æå¤±è´¥æˆ–æ— æœ‰æ•ˆæ•°æ®")
                
            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            time.sleep(1)
            
        except Exception as e:
            print(f"âŒ åˆ†æå‡ºé”™: {e}")
            continue
    
    print(f"\n" + "=" * 100)
    print(f"åˆ†æå®Œæˆï¼æˆåŠŸåˆ†æ {successful_count}/{len(journals_list)} ä¸ªæœŸåˆŠ")
    print("=" * 100)
    
    # ä¿å­˜ç»“æœ
    save_results(all_results)
    
    # ç”Ÿæˆæ’è¡Œæ¦œ
    generate_ranking_report(all_results)

def get_top_journals_list() -> List[str]:
    """è·å–é¡¶çº§æœŸåˆŠåˆ—è¡¨"""
    return [
        # Natureç³»åˆ—
        "Nature",
        "Nature Medicine",
        "Nature Biotechnology", 
        "Nature Genetics",
        "Nature Immunology",
        "Nature Neuroscience",
        "Nature Cell Biology",
        "Nature Communications",
        "Nature Reviews Molecular Cell Biology",
        "Nature Reviews Drug Discovery",
        "Nature Reviews Cancer",
        "Nature Reviews Immunology",
        "Nature Reviews Genetics",
        
        # Scienceç³»åˆ—
        "Science",
        "Science Translational Medicine",
        "Science Immunology",
        
        # Cellç³»åˆ—
        "Cell",
        "Cell Metabolism",
        "Cell Stem Cell",
        "Cancer Cell",
        "Molecular Cell",
        "Developmental Cell",
        "Current Biology",
        
        # åŒ»å­¦æœŸåˆŠ
        "The Lancet",
        "New England Journal of Medicine",
        "JAMA",
        "BMJ",
        "Annals of Internal Medicine",
        "The Lancet Oncology",
        
        # å…¶ä»–é¡¶çº§æœŸåˆŠ
        "PNAS",
        "eLife",
        "EMBO Journal",
        "Journal of Clinical Investigation",
        "Blood",
        "Immunity",
        "Neuron",
        "Cancer Research",
        "Journal of Experimental Medicine",
        "Genes & Development",
        "Molecular Biology of the Cell",
        "EMBO Reports",
        "Nature Structural & Molecular Biology",
        "Nature Chemical Biology",
        "Nature Methods",
        "Genome Research",
        "PLoS Biology",
        "Nucleic Acids Research",
        "Bioinformatics",
        "Genome Biology"
    ]

def analyze_journal_review_cycle(journal_name: str, max_papers: int = 200) -> Optional[Dict[str, Any]]:
    """åˆ†æå•ä¸ªæœŸåˆŠçš„å®¡ç¨¿å‘¨æœŸ"""
    
    # æ„å»ºæ£€ç´¢å¼ - è¿‘ä¸€å¹´çš„æ–‡çŒ®
    search_query = f'("{journal_name}"[Journal]) AND (("2024/01/01"[Date - Create] : "2025/12/31"[Date - Create]))'
    
    try:
        # æœç´¢æ–‡çŒ®
        pmids = search_papers(search_query, max_papers)
        if not pmids:
            return None
        
        print(f"   æ‰¾åˆ° {len(pmids)} ç¯‡æ–‡çŒ®")
        
        # åˆ†æ‰¹è·å–è¯¦ç»†ä¿¡æ¯
        papers_with_dates = []
        batch_size = 50
        
        for i in range(0, len(pmids), batch_size):
            batch = pmids[i:i + batch_size]
            batch_papers = fetch_papers_batch(batch)
            papers_with_dates.extend(batch_papers)
            
            if i + batch_size < len(pmids):
                time.sleep(0.5)  # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        
        if not papers_with_dates:
            return None
        
        print(f"   æˆåŠŸè§£æ {len(papers_with_dates)} ç¯‡æ–‡çŒ®çš„æ—¶é—´ä¿¡æ¯")
        
        # è®¡ç®—å®¡ç¨¿å‘¨æœŸ
        review_cycles = calculate_review_cycles_from_papers(papers_with_dates)
        
        if not review_cycles:
            return None
        
        # è®¡ç®—ç»Ÿè®¡å€¼
        result = {
            'journal_name': journal_name,
            'total_papers': len(papers_with_dates),
            'papers_with_review_data': len(review_cycles),
            'avg_review_days': round(statistics.mean(review_cycles), 1),
            'median_review_days': round(statistics.median(review_cycles), 1),
            'min_review_days': min(review_cycles),
            'max_review_days': max(review_cycles),
            'std_review_days': round(statistics.stdev(review_cycles) if len(review_cycles) > 1 else 0, 1),
            'review_cycles': review_cycles,
            'analysis_date': datetime.now().isoformat()
        }
        
        return result
        
    except Exception as e:
        print(f"   åˆ†æå¤±è´¥: {e}")
        return None

def search_papers(query: str, max_results: int) -> List[str]:
    """æœç´¢æ–‡çŒ®è·å–PMIDåˆ—è¡¨"""
    try:
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        params = {
            'db': 'pubmed',
            'term': query,
            'retmax': max_results,
            'retmode': 'xml',
            'tool': 'NNScholar',
            'email': 'test@nnscholar.com'
        }
        
        response = requests.get(base_url, params=params, timeout=30)
        response.raise_for_status()
        
        root = ET.fromstring(response.content)
        pmids = [id_elem.text for id_elem in root.findall('.//Id')]
        
        return pmids
        
    except Exception as e:
        return []

def fetch_papers_batch(pmids: List[str]) -> List[Dict[str, Any]]:
    """æ‰¹é‡è·å–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯"""
    try:
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
        params = {
            'db': 'pubmed',
            'id': ','.join(pmids),
            'retmode': 'xml',
            'rettype': 'abstract',
            'tool': 'NNScholar',
            'email': 'test@nnscholar.com'
        }
        
        response = requests.get(base_url, params=params, timeout=120)
        response.raise_for_status()
        
        return parse_papers_for_review_cycle(response.content)
        
    except Exception as e:
        return []

def parse_papers_for_review_cycle(xml_content: bytes) -> List[Dict[str, Any]]:
    """è§£ææ–‡çŒ®XMLè·å–å®¡ç¨¿å‘¨æœŸç›¸å…³ä¿¡æ¯"""
    papers = []
    
    try:
        root = ET.fromstring(xml_content)
        
        for article_elem in root.findall('.//PubmedArticle'):
            paper_data = extract_review_cycle_data(article_elem)
            if paper_data:
                papers.append(paper_data)
                
    except ET.ParseError:
        pass
    
    return papers

def extract_review_cycle_data(article_elem) -> Optional[Dict[str, Any]]:
    """æå–å•ç¯‡æ–‡çŒ®çš„å®¡ç¨¿å‘¨æœŸæ•°æ®"""
    try:
        # è·å–PMID
        medline_citation = article_elem.find('.//MedlineCitation')
        if medline_citation is None:
            return None
        
        pmid = get_text(medline_citation.find('.//PMID'))
        if not pmid:
            return None
        
        # æå–æ—¶é—´ä¿¡æ¯
        dates = {}
        
        # ä»PubmedData/Historyä¸­æå–å„ç§çŠ¶æ€çš„æ—¥æœŸ
        pubmed_data = article_elem.find('.//PubmedData')
        if pubmed_data is not None:
            history = pubmed_data.find('.//History')
            if history is not None:
                for pub_date in history.findall('.//PubMedPubDate'):
                    status = pub_date.get('PubStatus', '')
                    date_obj = parse_date_from_element(pub_date)
                    if date_obj:
                        dates[status] = date_obj
        
        # åªè¿”å›æœ‰receivedå’Œacceptedæ—¥æœŸçš„æ–‡çŒ®
        if 'received' in dates and 'accepted' in dates:
            return {
                'pmid': pmid,
                'received_date': dates['received'],
                'accepted_date': dates['accepted'],
                'other_dates': dates
            }
        
        return None
        
    except Exception:
        return None

def parse_date_from_element(date_elem) -> Optional[datetime]:
    """ä»XMLå…ƒç´ è§£ææ—¥æœŸ"""
    try:
        year = get_text(date_elem.find('.//Year'))
        month = get_text(date_elem.find('.//Month'))
        day = get_text(date_elem.find('.//Day'))
        
        if not year:
            return None
        
        # å¤„ç†æœˆä»½
        if month:
            try:
                month_num = int(month)
            except ValueError:
                # å¤„ç†æœˆä»½åç§°
                month_names = {
                    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
                    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12
                }
                month_num = month_names.get(month[:3], 1)
        else:
            month_num = 1
        
        day_num = int(day) if day else 1
        
        return datetime(int(year), month_num, day_num)
        
    except (ValueError, TypeError):
        return None

def calculate_review_cycles_from_papers(papers: List[Dict[str, Any]]) -> List[int]:
    """ä»æ–‡çŒ®æ•°æ®è®¡ç®—å®¡ç¨¿å‘¨æœŸ"""
    review_cycles = []
    
    for paper in papers:
        received_date = paper.get('received_date')
        accepted_date = paper.get('accepted_date')
        
        if received_date and accepted_date:
            # è®¡ç®—å¤©æ•°å·®
            days = (accepted_date - received_date).days
            
            # è¿‡æ»¤å¼‚å¸¸å€¼ï¼ˆ0-730å¤©èŒƒå›´å†…ï¼‰
            if 0 <= days <= 730:
                review_cycles.append(days)
    
    return review_cycles

def save_results(results: List[Dict[str, Any]]):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜JSONæ ¼å¼
    json_filename = f"all_journals_review_cycles_{timestamp}.json"
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump({
            'analysis_time': datetime.now().isoformat(),
            'total_journals_analyzed': len(results),
            'results': results
        }, f, ensure_ascii=False, indent=2, default=str)
    
    # ä¿å­˜CSVæ ¼å¼
    csv_filename = f"all_journals_review_cycles_{timestamp}.csv"
    with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow([
            'æœŸåˆŠåç§°', 'æ€»æ–‡çŒ®æ•°', 'æœ‰å®¡ç¨¿æ•°æ®æ–‡çŒ®æ•°', 'å¹³å‡å®¡ç¨¿å‘¨æœŸ(å¤©)', 
            'ä¸­ä½æ•°(å¤©)', 'æœ€çŸ­(å¤©)', 'æœ€é•¿(å¤©)', 'æ ‡å‡†å·®(å¤©)', 'æ•°æ®å®Œæ•´ç‡(%)'
        ])
        
        for result in results:
            data_completeness = (result['papers_with_review_data'] / result['total_papers']) * 100
            writer.writerow([
                result['journal_name'],
                result['total_papers'],
                result['papers_with_review_data'],
                result['avg_review_days'],
                result['median_review_days'],
                result['min_review_days'],
                result['max_review_days'],
                result['std_review_days'],
                f"{data_completeness:.1f}%"
            ])
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜:")
    print(f"   JSONæ ¼å¼: {json_filename}")
    print(f"   CSVæ ¼å¼: {csv_filename}")

def generate_ranking_report(results: List[Dict[str, Any]]):
    """ç”Ÿæˆæ’è¡Œæ¦œæŠ¥å‘Š"""
    if not results:
        return
    
    # æŒ‰å¹³å‡å®¡ç¨¿å‘¨æœŸæ’åº
    sorted_results = sorted(results, key=lambda x: x['avg_review_days'])
    
    print(f"\nğŸ“Š æœŸåˆŠå®¡ç¨¿å‘¨æœŸæ’è¡Œæ¦œ (Top {len(sorted_results)})")
    print("=" * 100)
    print(f"{'æ’å':<4} {'æœŸåˆŠåç§°':<40} {'å¹³å‡å‘¨æœŸ':<10} {'ä¸­ä½æ•°':<8} {'æ ·æœ¬æ•°':<8} {'å®Œæ•´ç‡':<8}")
    print("-" * 100)
    
    for i, result in enumerate(sorted_results, 1):
        data_completeness = (result['papers_with_review_data'] / result['total_papers']) * 100
        
        print(f"{i:<4} {result['journal_name']:<40} {result['avg_review_days']:<10.1f} "
              f"{result['median_review_days']:<8.1f} {result['papers_with_review_data']:<8} "
              f"{data_completeness:<8.1f}%")
    
    # ç»Ÿè®¡æ‘˜è¦
    avg_cycles = [r['avg_review_days'] for r in results]
    print(f"\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
    print(f"   åˆ†ææœŸåˆŠæ•°: {len(results)}")
    print(f"   å¹³å‡å®¡ç¨¿å‘¨æœŸ: {statistics.mean(avg_cycles):.1f} å¤©")
    print(f"   ä¸­ä½æ•°å®¡ç¨¿å‘¨æœŸ: {statistics.median(avg_cycles):.1f} å¤©")
    print(f"   æœ€å¿«æœŸåˆŠ: {sorted_results[0]['journal_name']} ({sorted_results[0]['avg_review_days']:.1f}å¤©)")
    print(f"   æœ€æ…¢æœŸåˆŠ: {sorted_results[-1]['journal_name']} ({sorted_results[-1]['avg_review_days']:.1f}å¤©)")

def get_text(element) -> str:
    """å®‰å…¨è·å–å…ƒç´ æ–‡æœ¬"""
    if element is None:
        return ''
    return ''.join(element.itertext()).strip()

if __name__ == "__main__":
    calculate_all_journals_review_cycles()
